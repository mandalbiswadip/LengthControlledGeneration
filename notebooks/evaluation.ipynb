{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12dba52",
   "metadata": {},
   "source": [
    "**Description:** Positional length control. No length prediction. Ground Truth length is used as input to decoder for absolute positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4efa135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8345bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ea6325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29711aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bd9350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de36e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24c1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f7e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11f88756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinusodial_positional_embedding import SinusoidalPositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1453cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import analysis_utils\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from dataset import CitationTextGenerationDataset, CitationTextGenerationDatasetNoCitationType\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from modelling_led import LEDForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e05226",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfaafd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 16384\n",
    "max_output_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75b4039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch, special_tokens=['[Dominant]', '[Reference]'], length=None):\n",
    "    # tokenize the inputs and labels\n",
    "    \n",
    "    additional_special_tokens_lookup = {token: idx for token, idx in zip(tokenizer.additional_special_tokens, tokenizer.additional_special_tokens_ids)}\n",
    "    special_token_ids = set([additional_special_tokens_lookup[token] for token in special_tokens])\n",
    "    special_token_ids.add(tokenizer.mask_token_id)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        batch[\"source\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_length,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "    outputs = tokenizer(\n",
    "        batch[\"target\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_output_length,\n",
    "        add_special_tokens=True \n",
    "    )\n",
    "    if length:\n",
    "        batch[\"length\"] = length\n",
    "    else:\n",
    "        batch[\"length\"] = [sum(x) for x in outputs.attention_mask]\n",
    "\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "\n",
    "    # create 0 global_attention_mask lists\n",
    "    batch[\"global_attention_mask\"] = len(batch[\"input_ids\"]) * [\n",
    "        [0 for _ in range(len(batch[\"input_ids\"][0]))]\n",
    "    ]\n",
    "\n",
    "    # since above lists are references, the following line changes the 0 index for all samples\n",
    "    for i_batch in range(len(batch[\"input_ids\"])):\n",
    "        for i_token in range(len(batch[\"input_ids\"][0])):\n",
    "            if batch[\"input_ids\"][i_batch][i_token] in special_token_ids:\n",
    "                batch[\"global_attention_mask\"][i_batch][i_token] = 1\n",
    "            \n",
    "    batch[\"labels\"] = outputs.input_ids\n",
    "\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
    "        for labels in batch[\"labels\"]\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a1137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/bxm200000/models/dominant_only/led_generations/positional_length_control_v8_dominant_only/checkpoint-67500/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0b5d334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "special_tokens = ['<doc>','</doc>', '[BOS]', '[Mask]', '[Dominant]', '[Reference]', '[B_Dominant]',  '[E_Dominant]', '[B_Reference]', '[E_Reference]']\n",
    "# special_tokens = ['<doc>','</doc>', '[BOS]', '[Mask]', '[B_Mask]',  '[E_Mask]']\n",
    "additional_special_tokens = {'additional_special_tokens': special_tokens}\n",
    "tokenizer.add_special_tokens(additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40df09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEDForConditionalGeneration.from_pretrained(\n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b268ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absolute'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.sinpostype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "707beedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDForConditionalGeneration(\n",
       "  (led): LEDModel(\n",
       "    (shared): Embedding(50275, 768, padding_idx=1)\n",
       "    (encoder): LEDEncoder(\n",
       "      (embed_tokens): Embedding(50275, 768, padding_idx=1)\n",
       "      (embed_positions): LEDLearnedPositionalEmbedding(16384, 768, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDEncoderLayer(\n",
       "          (self_attn): LEDEncoderAttention(\n",
       "            (longformer_self_attn): LEDEncoderSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (query_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_global): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): LEDDecoder(\n",
       "      (embed_tokens): Embedding(50275, 768, padding_idx=1)\n",
       "      (embed_positions): SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): LEDDecoderLayer(\n",
       "          (self_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): LEDDecoderAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50275, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device).half()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87672116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362/362 [00:08<00:00, 43.87it/s]\n"
     ]
    }
   ],
   "source": [
    "val_set = CitationTextGenerationDatasetNoCitationType(\n",
    "    \"/home/data/XiangciLi/CORWA/annotated_test/\", \n",
    "    tokenizer, \n",
    "    MAX_SENT_LEN = max_input_length,\n",
    "    related_work_path='/home/data/XiangciLi/20200705v1/acl/selected_related_work.jsonl',\n",
    "    cited_metadata_path='/home/data/XiangciLi/20200705v1/acl/selected_cited_metadata.jsonl',\n",
    "    cited_paper_path=\"/home/data/XiangciLi/20200705v1/acl/selected_cited_pdf_parses.jsonl\",\n",
    "    citing_paper_path=\"/home/data/XiangciLi/20200705v1/acl/selected_pdf_parses.jsonl\",\n",
    "    skip_no_citations = True,\n",
    "    include_intro = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "089b03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(src):\n",
    "    \"\"\"Get citations given source content\"\"\"\n",
    "    all_citations = []\n",
    "    for cite_data in src.split(\"[B_Reference]\")[1:]:\n",
    "\n",
    "        all_citations.append(cite_data.split(\"</s>\")[0].strip())\n",
    "\n",
    "    for cite_data in src.split(\"[B_Dominant]\")[1:]:\n",
    "\n",
    "        all_citations.append(cite_data.split(\"</s>\")[0].strip())\n",
    "        \n",
    "    for cite_data in src.split(\"[B_Mask]\")[1:]:\n",
    "\n",
    "        all_citations.append(cite_data.split(\"</s>\")[0].strip())\n",
    "    \n",
    "    return all_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d81599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_token_count(sent):\n",
    "    return tokenizer.tokenize(sent).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23a361ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set.filter_citation_type('Dominant')\n",
    "\n",
    "\n",
    "avg_1 = np.mean([get_sentence_token_count(x[\"target\"]) for x in val_set if len(get_citations(x[\"source\"]) ) == 1])\n",
    "avg_1 = int(avg_1)\n",
    "\n",
    "avg_2 = np.mean([get_sentence_token_count(x[\"target\"]) for x in val_set if len(get_citations(x[\"source\"]) ) == 2])\n",
    "avg_2 = int(avg_2)\n",
    "\n",
    "avg_greater_2 = np.mean([get_sentence_token_count(x[\"target\"]) for x in val_set if len(get_citations(x[\"source\"]) ) == 4])\n",
    "avg_greater_2 = int(avg_greater_2)\n",
    "\n",
    "all_lengths = [get_sentence_token_count(x[\"target\"]) for x in val_set]\n",
    "\n",
    "avg_length = int(np.mean(all_lengths))\n",
    "avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4f681ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_length():\n",
    "    return random.choice(all_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acc51d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(source):\n",
    "    return source.split(\"\\n\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "004d84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_sentence_avg_length(source):\n",
    "    context = get_context(source)\n",
    "    sentences = context.split(\"\\n\")\n",
    "    return np.mean([len(x) for x in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d21923fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desired_length(source):\n",
    "    cit_count = get_citations(source).__len__()\n",
    "    if cit_count ==1 :\n",
    "        return avg_1\n",
    "    elif cit_count == 2:\n",
    "        return avg_2\n",
    "    elif cit_count == 0:\n",
    "        print(\"Warning: citation count is 0\")\n",
    "        return avg_length\n",
    "    return avg_greater_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "994abb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_avg_length(idd):\n",
    "    return span_length_dict[get_id(idd)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c80f0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for sample in val_set.samples:\n",
    "    source = sample[\"source\"]\n",
    "    source = source.replace(\"[B_Mask]\", \"[B_Dominant]\")\n",
    "    source = source.replace(\"[E_Mask]\", \"[E_Dominant]\")\n",
    "    sample[\"source\"] = source\n",
    "    samples.append(sample)\n",
    "\n",
    "val_set.samples = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd27a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 775/1322 [40:24<27:01,  2.96s/it]  "
     ]
    }
   ],
   "source": [
    "#reference_predicted = []\n",
    "#reference_reference = []\n",
    "#dominant_predicted = []\n",
    "#dominant_reference = []\n",
    "accumulated_data = []               # store all generation data\n",
    "for batch in tqdm(DataLoader(val_set, batch_size = 1, shuffle=False)):\n",
    "    processed_batch = process_data_to_model_inputs(\n",
    "        batch, \n",
    "        special_tokens=['[Mask]'], \n",
    "        length=get_random_length()\n",
    "    )\n",
    "    processed_batch_cuda = {}\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\", \"length\"]:\n",
    "        processed_batch_cuda[key] = torch.tensor(processed_batch[key]).to(device)\n",
    "\n",
    "    model_kwargs = {'decoder_length' : processed_batch_cuda[\"length\"].unsqueeze(0)}\n",
    "    predicted_abstract_ids = model.generate(\n",
    "        input_ids = processed_batch_cuda[\"input_ids\"], \n",
    "        attention_mask=processed_batch_cuda[\"attention_mask\"], \n",
    "        global_attention_mask=processed_batch_cuda[\"global_attention_mask\"],\n",
    "        **model_kwargs,\n",
    "    )    \n",
    "    out = tokenizer.batch_decode(predicted_abstract_ids, skip_special_tokens=True)\n",
    "    target = batch[\"target\"]\n",
    "    for o, t, part_id, source in zip(out, target, batch[\"id\"], batch[\"source\"]):\n",
    "        accumulated_data.append(\n",
    "            {\"source\": source, \"target\": t, \n",
    "             \"generated\": o, \"part_id\": part_id}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, val_data in zip(accumulated_data, val_set):\n",
    "    data[\"citation_type\"] = val_data[\"citation_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2561b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919c0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_utils.write_json(\n",
    "    accumulated_data,\n",
    "    os.path.abspath(os.path.join(path, \"../sample_output_random_sampled_length.json\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a27ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = '/home/bxm200000/models/dominant_only/led_generations/span_v1_cdlm/sample_output.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc4b4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_data = analysis_utils.load_json(\n",
    "    json_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bef8be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulated_data = analysis_utils.load_json(\n",
    "#     os.path.abspath(os.path.join(path, \"../sample_output_length_ground_truth.json\"))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b24773",
   "metadata": {},
   "source": [
    "### Difference in Length of generation and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "304506f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length_difference(data, tokenizer=tokenizer):\n",
    "    \"\"\"\"\"\"\n",
    "    return tokenizer.tokenize(data[\"target\"]).__len__() - tokenizer.tokenize(data[\"generated\"]).__len__()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "047d6d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Target span Length - Generated span length.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff = [get_length_difference(x) for x in accumulated_data]\n",
    "\n",
    "plt.hist(diff, bins=50)\n",
    "\n",
    "plt.title(\n",
    "    \"Span generation. Mean ~ {} Median ~{}\".format(\n",
    "    sum(diff)/len(diff), np.median(diff))\n",
    ")\n",
    "plt.xlabel(\n",
    "    \"Target span Length - Generated span length.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76140c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dom_lengths = [x for x in ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0df896c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005037375189107414"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".001* analysis_utils.get_length_variance_from_data(accumulated_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c11c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([abs(tokenizer.tokenize(x[\"target\"]).__len__() - avg_length)  for x in accumulated_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a3c6a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "571567e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dominant_data, reference_data = analysis_utils.split_data_by_citation_type(accumulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efcbaac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = [analysis_utils.remove_citation_from_sentence(x[\"generated\"], x[\"source\"]) for x in accumulated_data]\n",
    "\n",
    "reference = [analysis_utils.remove_citation_from_sentence(x[\"target\"], x[\"source\"]) for x in accumulated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1b4ca1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' proposed a distillation-based distillation approach\\nThis distillation is based on the distillation of all the models in a single distillation step'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1dfa42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' proposed to fine tune a multilingual model to a specified zeroshot-direction with pseudo-parallel data'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89da05e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.2772599041339654, recall=0.29695541973873824, fmeasure=0.26425983896424876), mid=Score(precision=0.28583858716884747, recall=0.306074768321172, fmeasure=0.2715437557668595), high=Score(precision=0.2948596186021861, recall=0.3150998270500466, fmeasure=0.27830572083795424)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.07405053189002031, recall=0.0802334250553994, fmeasure=0.07051513933336478), mid=Score(precision=0.07914633992595807, recall=0.08672514801471393, fmeasure=0.07550899355923901), high=Score(precision=0.08442027235012582, recall=0.09318375124249138, fmeasure=0.08068082482299987)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.20936910638239783, recall=0.23191266977355193, fmeasure=0.2026179095483752), mid=Score(precision=0.21591750131711807, recall=0.23912317094283175, fmeasure=0.2081367007095653), high=Score(precision=0.2227267775115336, recall=0.24786492728810786, fmeasure=0.21431070065934266))}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(\n",
    "    predictions=predicted, \n",
    "    references=reference, \n",
    "    rouge_types=[\"rouge1\",\"rouge2\",\"rougeL\"],\n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36f2d70",
   "metadata": {},
   "source": [
    "### Changing Desired Length Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0da2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(batch, model, length=None):\n",
    "    processed_batch = process_data_to_model_inputs(\n",
    "        batch, \n",
    "        special_tokens=['[Dominant]', '[Reference]'],\n",
    "        length=length\n",
    "    )\n",
    "    processed_batch_cuda = {}\n",
    "    for key in [\"input_ids\", \"attention_mask\", \"global_attention_mask\", \"labels\", \"length\"]:\n",
    "        processed_batch_cuda[key] = torch.tensor(processed_batch[key]).to(device)\n",
    "    model_kwargs = {'decoder_length' : processed_batch_cuda[\"length\"].unsqueeze(0)}\n",
    "    predicted_abstract_ids = model.generate(\n",
    "        processed_batch_cuda[\"input_ids\"], \n",
    "        attention_mask=processed_batch_cuda[\"attention_mask\"], \n",
    "        global_attention_mask=processed_batch_cuda[\"global_attention_mask\"],\n",
    "        **model_kwargs,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate = True,\n",
    "        num_return_sequences = 1,\n",
    "        \n",
    "    )\n",
    "    out = tokenizer.batch_decode(predicted_abstract_ids.sequences, skip_special_tokens=True)\n",
    "    target = batch[\"target\"]\n",
    "    return out, target, predicted_abstract_ids.sequences_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eee89109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_length_controlled_prediction(\n",
    "    source, target = \"dummy target\", length=None\n",
    "):\n",
    "    \"\"\"high level function for length controlled generation\"\"\"\n",
    "    test_data = {\"source\": source, \"target\": target, \"id\": \"test_123\"}\n",
    "\n",
    "    test_data_list = [test_data]\n",
    "    \n",
    "    for batch in DataLoader(test_data_list, batch_size = 1, shuffle=False):\n",
    "        out, ta, seq_scores = run_model(\n",
    "            batch, model,\n",
    "            length=length\n",
    "        )\n",
    "        break\n",
    "    return out[0], seq_scores[0].cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da23869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1322/1322 [2:07:09<00:00,  5.77s/it]  \n"
     ]
    }
   ],
   "source": [
    "length_configs = [10, 20, 30, 50, 80]\n",
    "\n",
    "\n",
    "for data in tqdm(accumulated_data):\n",
    "    source = data[\"source\"]\n",
    "    \n",
    "    for len_option in length_configs:\n",
    "        if \"generated_{}\".format(len_option) not in data:\n",
    "            data[\"generated_{}\".format(len_option)], data[\"generated_{}_score\".format(len_option)] = run_length_controlled_prediction(\n",
    "                source, length=len_option\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be09c9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulated_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "668e5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.abspath(os.path.join(path, \"../vary_length_sample_output.json\")), 'w') as f:\n",
    "    json.dump(accumulated_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f48411",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc81b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\"Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017) , better encode syntactic properties of language , and correlate with electrophysiological responses in the human brain . However, these all require annotated syntactic trees for training. In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model p θ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximize the log marginal likelihood log p θ (x) = log z p θ (x, z). Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives (Klein and Manning, 2002) or priors (Johnson et al., 2007) . These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions. Like RNN language models, RNNGs make no independence assumptions. Instead they encode structural bias through operations that compose linguistic constituents. The lack of independence assumptions contributes to the strong language modeling performance of RNNGs, but make unsupervised learning challenging. First, marginalization is intractable. Second, the biases imposed by the RNNG are relatively weak compared to those imposed by models like PCFGs. There is little pressure for non-trivial tree structure to emerge during unsupervised RNNG (URNNG) learning. In this work, we explore a technique for handling intractable marginalization while also injecting inductive bias. Specifically we employ amortized variational inference (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014 ) with a structured inference network. Variational inference lets us tractably optimize a lower bound on the log marginal likelihood, while employing a structured inference network encourages non-trivial structure. In particular, a con-ditional random field (CRF) constituency parser (Finkel et al., 2008; Durrett and Klein, 2015) , which makes significant independence assumptions, acts as a guide on the generative model to learn meaningful trees through regularizing the posterior (Ganchev et al., 2010) . We experiment with URNNGs on English and Chinese and observe that they perform well as language models compared to their supervised counterparts and standard neural LMs. In terms of grammar induction, they are competitive with recently-proposed neural architectures that discover tree-like structures through gated attention (Shen et al., 2018) . Our results, along with other recent work on joint language modeling/structure learning with deep networks (Shen et al., 2018 (Shen et al., , 2019 Wiseman et al., 2018; Kawakami et al., 2018) , suggest that it is possible learn generative models of language that model the underlying data well (i.e. assign high likelihood to held-out data) and at the same time induce meaningful linguistic structure. </s> work (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018 Shen et al., , 2019 .\\nIn contrast,  [Dominant] \\nOur work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; , policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019) , or differentiable relaxations (Choi et al., 2018; Maillard and Clark, 2018) .\\n\\n [B_Dominant] Buys and Blunsom (2018) </s> Neural Syntactic Generative Models with Exact Marginalization | We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic programming over shiftreduce parsing and minimal RNN-based feature sets. Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our model achieves a stateof-the-art result among generative approaches. We also report empirical results on unsupervised syntactic models and their role in language modeling. We find that our model formulation of latent dependencies with exact marginalization do not lead to better intrinsic language modeling performance than vanilla RNNs, and that parsing accuracy is not correlated with language modeling perplexity in stack-based models. [E_Dominant]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc1af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d77ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\"Recurrent neural network grammars (RNNGs) (Dyer et al., 2016) model sentences by first generating a nested, hierarchical syntactic structure which is used to construct a context representation to be conditioned upon for upcoming words. Supervised RNNGs have been shown to outperform standard sequential language models, achieve excellent results on parsing (Dyer et al., 2016; Kuncoro et al., 2017) , better encode syntactic properties of language , and correlate with electrophysiological responses in the human brain . However, these all require annotated syntactic trees for training. In this work, we explore unsupervised learning of recurrent neural network grammars for language modeling and grammar induction. Work done while the first author was an intern at DeepMind. Code available at https://github.com/harvardnlp/urnng The standard setup for unsupervised structure learning is to define a generative model p θ (x, z) over observed data x (e.g. sentence) and unobserved structure z (e.g. parse tree, part-of-speech sequence), and maximize the log marginal likelihood log p θ (x) = log z p θ (x, z). Successful approaches to unsupervised parsing have made strong conditional independence assumptions (e.g. context-freeness) and employed auxiliary objectives (Klein and Manning, 2002) or priors (Johnson et al., 2007) . These strategies imbue the learning process with inductive biases that guide the model to discover meaningful structures while allowing tractable algorithms for marginalization; however, they come at the expense of language modeling performance, particularly compared to sequential neural models that make no independence assumptions. Like RNN language models, RNNGs make no independence assumptions. Instead they encode structural bias through operations that compose linguistic constituents. The lack of independence assumptions contributes to the strong language modeling performance of RNNGs, but make unsupervised learning challenging. First, marginalization is intractable. Second, the biases imposed by the RNNG are relatively weak compared to those imposed by models like PCFGs. There is little pressure for non-trivial tree structure to emerge during unsupervised RNNG (URNNG) learning. In this work, we explore a technique for handling intractable marginalization while also injecting inductive bias. Specifically we employ amortized variational inference (Kingma and Welling, 2014; Rezende et al., 2014; Mnih and Gregor, 2014 ) with a structured inference network. Variational inference lets us tractably optimize a lower bound on the log marginal likelihood, while employing a structured inference network encourages non-trivial structure. In particular, a con-ditional random field (CRF) constituency parser (Finkel et al., 2008; Durrett and Klein, 2015) , which makes significant independence assumptions, acts as a guide on the generative model to learn meaningful trees through regularizing the posterior (Ganchev et al., 2010) . We experiment with URNNGs on English and Chinese and observe that they perform well as language models compared to their supervised counterparts and standard neural LMs. In terms of grammar induction, they are competitive with recently-proposed neural architectures that discover tree-like structures through gated attention (Shen et al., 2018) . Our results, along with other recent work on joint language modeling/structure learning with deep networks (Shen et al., 2018 (Shen et al., , 2019 Wiseman et al., 2018; Kawakami et al., 2018) , suggest that it is possible learn generative models of language that model the underlying data well (i.e. assign high likelihood to held-out data) and at the same time induce meaningful linguistic structure. </s> work (Cho et al., 2014; Chung et al., 2017; Shen et al., 2018 Shen et al., , 2019 .\\nIn contrast, Buys and Blunsom (2018) make Markov assumptions and perform exact marginalization over latent dependency trees.\\nOur work is also related to the recent line of work on learning latent trees as part of a deep model through supervision on other tasks, typically via differentiable structured hidden layers (Kim et al., 2017; Bradbury and Socher, 2017; Tran and Bisk, 2018; Peng et al., 2018; Niculae et al., 2018; , policy gradient-based approaches (Yogatama et al., 2017; Williams et al., 2018; Havrylov et al., 2019) , or  [Reference]  .\\n\\n [B_Reference] (Choi et al., 2018; </s> Learning to Compose Task-Specific Tree Structures | For years, recursive neural networks (RvNNs) have been shown to be suitable for representing text into fixed-length vectors and achieved good performance on several natural language processing tasks. However, the main drawback of RvNNs is that they require structured input, which makes data preparation and model implementation hard. In this paper, we propose Gumbel Tree-LSTM, a novel treestructured long short-term memory architecture that efficiently learns how to compose task-specific tree structures only from plain text data. Our model uses Straight-Through Gumbel-Softmax estimator to decide the parent node among candidates dynamically and to calculate gradients of the discrete decision. We evaluate the proposed model on natural language inference and sentiment analysis, and show that our model outperforms or is at least comparable to previous models. We also find that our model converges significantly faster than other models. [E_Reference] [B_Reference] Maillard and Clark, 2018) </s> Latent Tree Learning with Differentiable Parsers: Shift-Reduce Parsing and Chart Parsing | Latent tree learning models represent sentences by composing their words according to an induced parse tree, all based on a downstream task. These models often outperform baselines which use (externally provided) syntax trees to drive the composition order. This work contributes (a) a new latent tree learning model based on shift-reduce parsing, with competitive downstream performance and non-trivial induced trees, and (b) an analysis of the trees learned by our shift-reduce model and by a chart-based model. [E_Reference]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cc2b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c843ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_length_controlled_prediction(source, length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c586e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
