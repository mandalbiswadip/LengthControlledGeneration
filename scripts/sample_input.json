{
	"related_work_context": "Previous studies have explored joint modeling (Miwa and Bansal, 2016; Zhang et al., 2017; Singh et al., 2013; Yang and Mitchell, 2016) ) and multi-task learning (Peng and Dredze, 2015; Peng et al., 2017; Luan et al., 2018a Luan et al., , 2017a as methods to share representational strength across related information extraction tasks.\nThe most similar to ours is the work in [Mask] \nIn contrast, DYGIE uses dynamic graph propagation to explicitly incorporate rich contextual information into the span representations. ",
	"cited_papers": [

		{
			"paper_id": 1,
			"citation_type": "Dominant",
			"citation_mark": "Luan et al. (2018a)",
			"title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction",
			"abstract": "We introduce a multi-task setup of identifying and classifying entities, relations, and coreference clusters in scientific articles. We create SCIERC, a dataset that includes annotations for all three tasks and develop a unified framework called Scientific Information Extractor (SCIIE) for with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature."
		}
	]


}